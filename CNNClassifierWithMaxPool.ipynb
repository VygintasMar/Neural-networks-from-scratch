{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VygintasMar/Neural-networks-from-scratch/blob/main/CNNClassifierWithMaxPool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_BpyJ7dTQH1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import signal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sR0AwyN3TdXn"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    def __init__(self):\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        # TODO: return output\n",
        "        pass\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        # TODO: update parameters and return input gradient\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EClXzSSTenV"
      },
      "source": [
        "layer architecture below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgOLgu10TjnP"
      },
      "outputs": [],
      "source": [
        "class Convolutional(Layer):\n",
        "    def __init__(self, input_shape, kernel_size, depth):\n",
        "        input_depth, input_height, input_width = input_shape\n",
        "        self.depth = depth\n",
        "        self.input_shape = input_shape\n",
        "        self.input_depth = input_depth\n",
        "        self.output_shape = (depth, input_height - kernel_size + 1, input_width - kernel_size + 1)\n",
        "        self.kernels_shape = (depth, input_depth, kernel_size, kernel_size)\n",
        "        self.kernels = np.random.randn(*self.kernels_shape)\n",
        "        self.biases = np.random.randn(*self.output_shape)\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "\n",
        "        self.output = np.copy(self.biases)\n",
        "        for i in range(self.depth):\n",
        "            for j in range(self.input_depth):\n",
        "                self.output[i] += signal.correlate2d(self.input[j], self.kernels[i, j], \"valid\")\n",
        "\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        kernels_gradient = np.zeros(self.kernels_shape)\n",
        "        input_gradient = np.zeros(self.input_shape)\n",
        "\n",
        "\n",
        "        for i in range(self.depth):\n",
        "            for j in range(self.input_depth):\n",
        "                kernels_gradient[i, j] = signal.correlate2d(self.input[j], output_gradient[i], \"valid\")\n",
        "                input_gradient[j] += signal.convolve2d(output_gradient[i], self.kernels[i, j], \"full\")\n",
        "\n",
        "        self.kernels -= learning_rate * kernels_gradient\n",
        "        self.biases -= learning_rate * output_gradient\n",
        "        return input_gradient\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4z9cA7ITrc4"
      },
      "outputs": [],
      "source": [
        "class Dense(Layer):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = np.random.randn(output_size, input_size)\n",
        "        self.bias = np.random.randn(output_size, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        #print(np.shape(input), 'shape before dense')\n",
        "        return np.dot(self.weights, self.input) + self.bias\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        weights_gradient = np.dot(output_gradient, self.input.T)\n",
        "        input_gradient = np.dot(self.weights.T, output_gradient)\n",
        "        self.weights -= learning_rate * weights_gradient\n",
        "        self.bias -= learning_rate * output_gradient\n",
        "        return input_gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kgal-CufT1Q7"
      },
      "outputs": [],
      "source": [
        "class Reshape(Layer):\n",
        "    def __init__(self, input_shape, output_shape):\n",
        "        self.input_shape = input_shape\n",
        "        self.output_shape = output_shape\n",
        "\n",
        "    def forward(self, input):\n",
        "        #print(np.shape(input), 'shape before reshape')\n",
        "        #print(np.shape(np.reshape(input, self.output_shape)), 'shape after reshape')\n",
        "        return np.reshape(input, self.output_shape)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        #print(np.shape(input), 'shape after reshape')\n",
        "        return np.reshape(output_gradient, self.input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MaxPool:\n",
        "    def __init__(self, pool_size, stride):\n",
        "        self.pool_size = pool_size  # Size of the pooling window (e.g., 2 for 2x2)\n",
        "        self.stride = stride  # Stride with which the window moves across the input\n",
        "        self.cache = None  # Cache to store information needed for the backward pass\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Check if the input has 3 dimensions and reshape it to have a single channel\n",
        "\n",
        "        if X.ndim == 3:\n",
        "            X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
        "\n",
        "        self.cache = X  # Store the input value for use in the backward pass\n",
        "\n",
        "        n, h, w, c = X.shape\n",
        "        h_out = 1 + (h - self.pool_size) // self.stride\n",
        "        w_out = 1 + (w - self.pool_size) // self.stride\n",
        "\n",
        "        output = np.zeros((n, h_out, w_out, c))\n",
        "\n",
        "        for i in range(h_out):\n",
        "            for j in range(w_out):\n",
        "                h_start = i * self.stride\n",
        "                h_end = h_start + self.pool_size\n",
        "                w_start = j * self.stride\n",
        "                w_end = w_start + self.pool_size\n",
        "\n",
        "                output[:, i, j, :] = np.max(X[:, h_start:h_end, w_start:w_end, :], axis=(1, 2))\n",
        "\n",
        "        # Reshape the output back to 3 dimensions if the original input was 3D\n",
        "        if X.shape[3] == 1:\n",
        "            output = output.reshape(n, h_out, w_out)\n",
        "\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "    def backward(self, d_out, learning_rate):\n",
        "\n",
        "        X = self.cache\n",
        "\n",
        "        n, h, w, c = X.shape\n",
        "        h_out, w_out, _ = d_out.shape\n",
        "\n",
        "        d_X = np.zeros_like(X)\n",
        "\n",
        "        for i in range(h_out):\n",
        "          for j in range(w_out):\n",
        "              h_start = i * self.stride\n",
        "              w_start = j * self.stride\n",
        "              h_end = min(h_start + self.pool_size, h)\n",
        "              w_end = min(w_start + self.pool_size, w)\n",
        "\n",
        "              for c_i in range(c):  # Assuming 'c' is the number of channels\n",
        "                  a = X[h_start:h_end, w_start:w_end, c_i]\n",
        "                  if a.size > 0:  # Ensure 'a' is not empty\n",
        "                      max_idx = np.unravel_index(np.argmax(a, axis=None), a.shape)\n",
        "                      d_X[h_start:max_idx[0]+h_start, w_start:max_idx[1]+w_start, c_i] += d_out[i, j, c_i]\n",
        "\n",
        "        return d_X\n"
      ],
      "metadata": {
        "id": "ob5LT0tKE28B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G_7yh-sT6NC"
      },
      "source": [
        "activation below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9u1FJraT85x"
      },
      "outputs": [],
      "source": [
        "class Activation(Layer):\n",
        "    def __init__(self, activation, activation_prime):\n",
        "        self.activation = activation\n",
        "        self.activation_prime = activation_prime\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "\n",
        "        return self.activation(self.input)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        return np.multiply(output_gradient, self.activation_prime(self.input))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TihkzYF7UDPT"
      },
      "outputs": [],
      "source": [
        "class Tanh(Activation):\n",
        "    def __init__(self):\n",
        "        def tanh(x):\n",
        "            return np.tanh(x)\n",
        "\n",
        "        def tanh_prime(x):\n",
        "            return 1 - np.tanh(x) ** 2\n",
        "\n",
        "        super().__init__(tanh, tanh_prime)\n",
        "\n",
        "class Sigmoid(Activation):\n",
        "    def __init__(self):\n",
        "        def sigmoid(x):\n",
        "            return 1 / (1 + np.exp(-x))\n",
        "\n",
        "        def sigmoid_prime(x):\n",
        "            s = sigmoid(x)\n",
        "            return s * (1 - s)\n",
        "\n",
        "        super().__init__(sigmoid, sigmoid_prime)\n",
        "\n",
        "class Softmax(Layer):\n",
        "    def forward(self, input):\n",
        "        tmp = np.exp(input)\n",
        "        self.output = tmp / np.sum(tmp)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        # This version is faster than the one presented in the video\n",
        "        n = np.size(self.output)\n",
        "        return np.dot((np.identity(n) - self.output.T) * self.output, output_gradient)\n",
        "        # Original formula:\n",
        "        # tmp = np.tile(self.output, n)\n",
        "        # return np.dot(tmp * (np.identity(n) - np.transpose(tmp)), output_gradient)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Bmh-TczUI5E"
      },
      "source": [
        "errors below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PIxRbdGUKJQ"
      },
      "outputs": [],
      "source": [
        "def mse(y_true, y_pred):\n",
        "\n",
        "    return np.mean(np.power(y_true - y_pred, 2))\n",
        "\n",
        "def mse_prime(y_true, y_pred):\n",
        "    return 2 * (y_pred - y_true) / np.size(y_true)\n",
        "\n",
        "def binary_cross_entropy(y_true, y_pred):\n",
        "    return np.mean(-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "def binary_cross_entropy_prime(y_true, y_pred):\n",
        "    return ((1 - y_true) / (1 - y_pred) - y_true / y_pred) / np.size(y_true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em-w6KJaUke2"
      },
      "source": [
        "training loop below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRxfCYLzUrx4"
      },
      "outputs": [],
      "source": [
        "def predict(network, input):\n",
        "    output = input\n",
        "    for layer in network:\n",
        "        output = layer.forward(output)\n",
        "    return output\n",
        "\n",
        "def train(network, loss, loss_prime, x_train, y_train, epochs = 1000, learning_rate = 0.1, verbose = True):\n",
        "    for e in range(epochs):\n",
        "        error = 0\n",
        "        print(e, \"  epoch    \")\n",
        "        for x, y in zip(x_train, y_train):\n",
        "            # forward\n",
        "            att=0\n",
        "            output = predict(network, x)\n",
        "\n",
        "\n",
        "            # error\n",
        "            error += loss(y, output)\n",
        "            #print(error, \"  error  \")\n",
        "\n",
        "            # backward\n",
        "            grad = loss_prime(y, output)\n",
        "            ctt=0\n",
        "            for layer in reversed(network):\n",
        "                grad = layer.backward(grad, learning_rate)\n",
        "                #print(grad, 'ep ', ctt)\n",
        "                ctt+=1\n",
        "\n",
        "        error /= len(x_train)\n",
        "        if verbose:\n",
        "            print(f\"{e + 1}/{epochs}, error={error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDtTbw1CUueX"
      },
      "source": [
        "procces data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYalg6AcUwDg"
      },
      "outputs": [],
      "source": [
        "#!unzip '/content/sample_data/train.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oZ6nLKvWZCb"
      },
      "outputs": [],
      "source": [
        "dataDir = '/content/sample_data/tr'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnVPGR0VWgHC"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "for folder in os.listdir(dataDir):\n",
        "    if os.path.isdir(os.path.join(dataDir, folder)):\n",
        "        for image_filename in os.listdir(os.path.join(dataDir, folder)):\n",
        "            image_path = os.path.join(dataDir, folder, image_filename)\n",
        "            image = Image.open(image_path).resize((32, 32))\n",
        "            image = img_to_array(image)  # Convert image to numpy array\n",
        "            X.append(image)\n",
        "            y.append(folder)  # Using folder name as label\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7CXuls0bXFf",
        "outputId": "98206e33-971e-40fe-f998-c1f609c9e04f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(240, 3, 32, 32)\n",
            "[[[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]]\n",
            "[[[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "X = np.array(X)/255\n",
        "X_train = X.reshape(len(X), 3, 32, 32)\n",
        "\n",
        "\n",
        "y = np.array(y)\n",
        "y = np.array(y).reshape(-1, 1)\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y_encoded = encoder.fit_transform(y)\n",
        "print(np.shape(X_train))\n",
        "\n",
        "y_train = y_encoded.reshape(len(y_encoded), 2, 1)\n",
        "\n",
        "permutation = np.random.permutation(len(X_train))\n",
        "\n",
        "# Apply the permutation to X_train and y_train\n",
        "X_train = X_train[permutation]\n",
        "y_train = y_train[permutation]\n",
        "\n",
        "print(y_train)\n",
        "\n",
        "#for testing\n",
        "\n",
        "testDir='/content/sample_data/test'\n",
        "\n",
        "a=[]\n",
        "b=[]\n",
        "\n",
        "for folder in os.listdir(testDir):\n",
        "    if os.path.isdir(os.path.join(testDir, folder)):\n",
        "        for image_filename in os.listdir(os.path.join(testDir, folder)):\n",
        "            image_path = os.path.join(testDir, folder, image_filename)\n",
        "            image = Image.open(image_path).resize((32, 32))\n",
        "            image = img_to_array(image)  # Convert image to numpy array\n",
        "            a.append(image)\n",
        "            b.append(folder)  # Using folder name as label\n",
        "\n",
        "a = np.array(a)/255\n",
        "b = np.array(b).reshape(-1, 1)\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "b_encoded = encoder.fit_transform(b)\n",
        "\n",
        "\n",
        "y_test = b_encoded.reshape(len(b_encoded), 2, 1)\n",
        "X_test = a.reshape(len(a), 3, 32, 32)\n",
        "print(y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgUJSZ7QhBE6"
      },
      "source": [
        "network arch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qGH1xEfhDd5"
      },
      "outputs": [],
      "source": [
        "network = [\n",
        "    Convolutional((3, 32, 32), 3, 5),\n",
        "    maxPool(3,3),\n",
        "    Sigmoid(),\n",
        "    Convolutional((5, 30, 30), 3, 5),\n",
        "    Sigmoid(),\n",
        "    maxPool(3,3),\n",
        "    Reshape((5, 28, 28), (5 * 28 * 28, 1)),\n",
        "    Dense(5 * 28 * 28, 100),\n",
        "    Tanh(),\n",
        "    Dense(100, 2),\n",
        "    Softmax()\n",
        "]\n",
        "\n",
        "# train\n",
        "train(\n",
        "    network,\n",
        "    binary_cross_entropy,\n",
        "    binary_cross_entropy_prime,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=100,\n",
        "    learning_rate=0.1\n",
        ")\n",
        "\n",
        "# test\n",
        "for x, y in zip(X_test, y_test):\n",
        "    output = predict(network, x)\n",
        "    print(f\"pred: {np.argmax(output)}, true: {np.argmax(y)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZC9yANFYD_sC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPn+OP2ywzZbOyYzE7xrRn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}